{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the data"
      ],
      "metadata": {
        "id": "3AROg8ijC4Sp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "E89iu0XzCvR0",
        "outputId": "269f5a55-9998-44b3-dbc4-03f2f5d817e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-030d368c8185>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/drive/MyDrive/S-1/Test/{file_name}'\u001b[0m  \u001b[0;31m# Replace with the actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnormalized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_normalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mcommon_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-030d368c8185>\u001b[0m in \u001b[0;36mload_and_normalize_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_normalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Normalize the data using z-score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/S-1/Test/1.txt'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def load_and_normalize_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None)\n",
        "\n",
        "    # Normalize the data using z-score\n",
        "    scaler = StandardScaler()\n",
        "    normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "#give the required file name to be normalized\n",
        "file_names = ['1.txt']\n",
        "\n",
        "\n",
        "common_data = pd.DataFrame()\n",
        "\n",
        "for file_name in file_names:\n",
        "    file_path = f'/content/drive/MyDrive/S-1/Test/{file_name}'  # Replace with the actual path\n",
        "    normalized_data = load_and_normalize_data(file_path)\n",
        "    common_data = common_data.append(pd.DataFrame(normalized_data), ignore_index=True)\n",
        "\n",
        "# change the name to required file name after normalization\n",
        "common_data.to_csv('/content/drive/MyDrive/S-1/Normal data/S1_2_test_normal.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAV15Bv0tX1d",
        "outputId": "c5ef626f-5755-4a1c-e0c3-a67be1cb8525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Threshold Calculation"
      ],
      "metadata": {
        "id": "OrxJIhUGDGyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path, time_steps=10):\n",
        "    data = pd.read_csv(file_path, header=None)\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "def calculate_threshold(X_train, model, percentile=95):\n",
        "\n",
        "    reconstructions = model.predict(X_train)\n",
        "    mse = np.mean(np.square(X_train - reconstructions), axis=(1, 2))\n",
        "\n",
        "\n",
        "    threshold = np.percentile(mse, percentile)\n",
        "    return threshold\n",
        "\n",
        "\n",
        "def train_lstm_autoencoder(X_train, epochs=10, batch_size=32):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(RepeatVector(X_train.shape[1]))\n",
        "    model.add(LSTM(units=64, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(X_train.shape[2])))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# give the correct normalized train file path\n",
        "train_file_path = r'/content/drive/MyDrive/S-1/Normal data/S1_3_train_normal.csv'\n",
        "X_train = load_and_preprocess_data(train_file_path)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "\n",
        "lstm_autoencoder_model = train_lstm_autoencoder(X_train)\n",
        "\n",
        "\n",
        "threshold = calculate_threshold(X_train, lstm_autoencoder_model)\n",
        "\n",
        "print(f\"Threshold: {threshold}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymNmSg-jDJkH",
        "outputId": "1eafcbf3-e7df-45f8-d1b1-9c04aacbb3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2372/2372 [==============================] - 136s 54ms/step - loss: 0.5599\n",
            "Epoch 2/10\n",
            "2372/2372 [==============================] - 127s 54ms/step - loss: 0.5080\n",
            "Epoch 3/10\n",
            "2372/2372 [==============================] - 126s 53ms/step - loss: 0.4742\n",
            "Epoch 4/10\n",
            "2372/2372 [==============================] - 125s 53ms/step - loss: 0.4320\n",
            "Epoch 5/10\n",
            "2372/2372 [==============================] - 125s 52ms/step - loss: 0.3955\n",
            "Epoch 6/10\n",
            "2372/2372 [==============================] - 126s 53ms/step - loss: 0.3968\n",
            "Epoch 7/10\n",
            "2372/2372 [==============================] - 132s 56ms/step - loss: 0.3269\n",
            "Epoch 8/10\n",
            "2372/2372 [==============================] - 133s 56ms/step - loss: 0.2939\n",
            "Epoch 9/10\n",
            "2372/2372 [==============================] - 131s 55ms/step - loss: 0.2743\n",
            "Epoch 10/10\n",
            "2372/2372 [==============================] - 134s 56ms/step - loss: 0.2772\n",
            "2372/2372 [==============================] - 45s 19ms/step\n",
            "Threshold: 0.44315840680685453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "import pandas as pd\n",
        "\n",
        "def save_predicted_labels(labels, output_path):\n",
        "    df = pd.DataFrame({'Anomaly': labels})\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path, time_steps=10):\n",
        "    data = pd.read_csv(file_path, header=None)\n",
        "\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps + 1):\n",
        "        X.append(data.values[i:i + time_steps, :])\n",
        "        y.append(data.values[i + time_steps - 1, :])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "def detect_anomalies(model, X_test, threshold=0.5):\n",
        "\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "    reconstructions = model.predict(X_test_reshaped)\n",
        "\n",
        "\n",
        "    mse = np.mean(np.square(X_test_reshaped - reconstructions), axis=(1, 2))\n",
        "\n",
        "\n",
        "    anomalies = (mse > threshold).astype(int)\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "def train_lstm_autoencoder(X_train, epochs=10, batch_size=32):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(RepeatVector(X_train.shape[1]))\n",
        "    model.add(LSTM(units=64, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(X_train.shape[2])))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    return model\n",
        "\n",
        "# give the correct normalized train file path\n",
        "train_file_path = r'/content/drive/MyDrive/S-1/Normal data/S1_3_train_normal.csv'\n",
        "X_train, y_train = load_and_preprocess_data(train_file_path)\n",
        "\n",
        "# give the correct normalized test file path\n",
        "test_file_path = r'/content/drive/MyDrive/S-1/Normal data/S1_3_test_normal.csv'\n",
        "X_test, y_test = load_and_preprocess_data(test_file_path)\n",
        "\n",
        "\n",
        "lstm_autoencoder_model = train_lstm_autoencoder(X_train)\n",
        "\n",
        "\n",
        "anomalies = detect_anomalies(lstm_autoencoder_model, X_test, threshold=0.443)  # Adjust threshold\n",
        "\n",
        "# Save predicted labels to a CSV file\n",
        "output_csv_path = r'/content/drive/MyDrive/S-1/Pre_lab/3_predicted_labels_lstm.csv'\n",
        "save_predicted_labels(anomalies, output_csv_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD8W7MnFDUiF",
        "outputId": "1d12faf5-ecf3-42c6-d2d9-bebf22df1861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "237/237 [==============================] - 18s 58ms/step - loss: 0.3399\n",
            "Epoch 2/5\n",
            "237/237 [==============================] - 10s 41ms/step - loss: 0.2436\n",
            "Epoch 3/5\n",
            "237/237 [==============================] - 13s 53ms/step - loss: 0.2190\n",
            "Epoch 4/5\n",
            "237/237 [==============================] - 13s 57ms/step - loss: 0.2064\n",
            "Epoch 5/5\n",
            "237/237 [==============================] - 10s 41ms/step - loss: 0.1995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "741/741 [==============================] - 8s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Measures"
      ],
      "metadata": {
        "id": "xhR1PTSdDZjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def count_ones_and_matches(file1_path, file2_path):\n",
        "    ones_count_file1 = 0\n",
        "    zero_count_file1 = 0\n",
        "    zero_count_file2 = 0\n",
        "    ones_count_file2 = 0\n",
        "    correct_matches_count_1 = 0\n",
        "    correct_matches_count_0 = 0\n",
        "\n",
        "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "        csv_reader1 = csv.reader(file1)\n",
        "        csv_reader2 = csv.reader(file2)\n",
        "\n",
        "\n",
        "        csv_data1 = [row for row in csv_reader1]\n",
        "        csv_data2 = [row for row in csv_reader2]\n",
        "\n",
        "\n",
        "    for row in csv_data1:\n",
        "        ones_count_file1 += row.count('1')\n",
        "        zero_count_file1 += row.count('0')\n",
        "\n",
        "    for row in csv_data2:\n",
        "        ones_count_file2 += row.count('1')\n",
        "        zero_count_file2 += row.count('0')\n",
        "\n",
        "\n",
        "    for row1, row2 in zip(csv_data1, csv_data2):\n",
        "        correct_matches_count_1 += sum(1 for a, b in zip(row1, row2) if a == b == '1')\n",
        "        correct_matches_count_0 += sum(1 for a, b in zip(row1, row2) if a == b == '0')\n",
        "\n",
        "    precision = correct_matches_count_0 / zero_count_file1\n",
        "    recall = correct_matches_count_0 / (correct_matches_count_0 + (ones_count_file1 - correct_matches_count_1))\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 score: {f1_score}\")\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "\n",
        "average_precision = 0\n",
        "average_recall = 0\n",
        "average_f1_score = 0\n",
        "\n",
        "for i in range(1, 8):\n",
        "    # give the predicted labels file path\n",
        "    csv_file1_path = f'/content/drive/MyDrive/S-1/Pre_lab/{i}_predicted_labels_lstm.csv'\n",
        "    # give the actual labels file path\n",
        "    csv_file2_path = f'/content/drive/MyDrive/S-1/Test_Label/{i}.csv'\n",
        "\n",
        "    print(f\"\\nEvaluation for iteration {i}:\")\n",
        "    precision, recall, f1_score = count_ones_and_matches(csv_file1_path, csv_file2_path)\n",
        "\n",
        "    average_precision += precision\n",
        "    average_recall += recall\n",
        "    average_f1_score += f1_score\n",
        "\n",
        "#change the denominator to number of files in that particular set\n",
        "average_precision /= 7\n",
        "average_recall /= 7\n",
        "average_f1_score /= 7\n",
        "\n",
        "print(\"\\nAverage Precision:\", average_precision)\n",
        "print(\"Average Recall:\", average_recall)\n",
        "print(\"Average F1 score:\", average_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Ne8cEdDb5N",
        "outputId": "88fc7246-d47e-4da4-afd6-d06db2579695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation for iteration 1:\n",
            "Precision: 0.9229162916291629\n",
            "Recall: 0.9944524188229817\n",
            "F1 score: 0.9573498655512399\n",
            "\n",
            "Evaluation for iteration 2:\n",
            "Precision: 0.9869449378330373\n",
            "Recall: 0.9603352920843415\n",
            "F1 score: 0.9734583041345479\n",
            "\n",
            "Evaluation for iteration 3:\n",
            "Precision: 0.9660979877515311\n",
            "Recall: 0.9653800760589238\n",
            "F1 score: 0.9657388984848154\n",
            "\n",
            "Evaluation for iteration 4:\n",
            "Precision: 0.9697442566103164\n",
            "Recall: 0.9736269475150143\n",
            "F1 score: 0.9716817234190409\n",
            "\n",
            "Evaluation for iteration 5:\n",
            "Precision: 0.996728992618132\n",
            "Recall: 0.9555875746916981\n",
            "F1 score: 0.9757247944612722\n",
            "\n",
            "Evaluation for iteration 6:\n",
            "Precision: 0.8466141593699125\n",
            "Recall: 0.9633987582615662\n",
            "F1 score: 0.9012389048923861\n",
            "\n",
            "Evaluation for iteration 7:\n",
            "Precision: 0.9556053401609363\n",
            "Recall: 0.9816824010145132\n",
            "F1 score: 0.9684683641081483\n",
            "\n",
            "Average Precision: 0.949235995139004\n",
            "Average Recall: 0.9706376383498627\n",
            "Average F1 score: 0.959094407864493\n"
          ]
        }
      ]
    }
  ]
}